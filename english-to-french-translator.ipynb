{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install libraries and datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36faaf859d449bb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install libraries\n",
    "Google's framework to build, train and deploy machine learning models at scale."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c4389ff9a15e5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Libraries for data analysis and visualization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfdaee191d86ea53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install plotly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad2a02f2005ec39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06ef6b7367b8cf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.src.layers import Embedding, LSTM, Dense, RepeatVector, TimeDistributed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed7f841cd8c9c0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7d686529f4e5cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_english = pd.read_csv('datasets/small_vocab_en.csv', header=None, sep='/t', names=['english'], engine='python')\n",
    "df_french = pd.read_csv('datasets/small_vocab_fr.csv', header=None, sep='/t', names=['french'], engine='python')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ae7f89a8b0913a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_english"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a2881866f3eed06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_french"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325e1336f51d7c07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check datasets\n",
    "1. Check how many samples are in each dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d0e802ee1b8a9b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_eng_samples = df_english.shape[0]\n",
    "number_of_fr_samples = df_french.shape[0]\n",
    "\n",
    "print('Number of English samples: {}'.format(number_of_eng_samples))\n",
    "print('Number of French samples: {}'.format(number_of_fr_samples))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c43dee2203cd9ca8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Check if there are any Null values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63f2cdc29363549e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of Null values in English dataset: \" + str(df_english.isnull().sum().sum()))\n",
    "print(\"Number of Null values in French dataset: \" + str(df_french.isnull().sum().sum()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac5b01972c5e541"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Check memory usage of each dataset in percentage."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1c197665a967ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_english.info()\n",
    "df_french.info()\n",
    "print(\"Memory usage of English dataset: \" + str(df_english.memory_usage().sum()) + ' bytes')\n",
    "print(\"Memory usage of French dataset: \" + str(df_french.memory_usage().sum()) + ' bytes')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c042b32e7c0fe1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Concatenate both datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0ab6a4b4b2fa4ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_both = pd.concat([df_english, df_french], axis=1)\n",
    "df_both"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12bd8654b109be7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfbfaa6b3d7e51b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove non-alphabetical characters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2628da9b8f15234"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcc216d46d1edd20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_both['english'] = df_both['english'].apply(remove_non_alphabetical_characters)\n",
    "df_both['french'] = df_both['french'].apply(remove_non_alphabetical_characters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "804bf82382992763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_both"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f46012540ee1a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check how many unique words are in each dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a377f29887a3fe4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_unique_words(text, set_of_words):\n",
    "    for word in text.split():\n",
    "        set_of_words.add(word)\n",
    "    return set_of_words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c0e400410347f8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_of_unique_eng_words = set()\n",
    "set_of_unique_fr_words = set()\n",
    "\n",
    "df_both['english'].apply(lambda x: get_unique_words(x, set_of_unique_eng_words))\n",
    "df_both['french'].apply(lambda x: get_unique_words(x, set_of_unique_fr_words))\n",
    "\n",
    "print('Number of unique words in English dataset: {}'.format(len(set_of_unique_eng_words)))\n",
    "print('Number of unique words in French dataset: {}'.format(len(set_of_unique_fr_words)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb8b255ec2e3f77d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print unique words in each dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "314f28fcb22e13e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Unique words in English dataset: {}'.format(set_of_unique_eng_words))\n",
    "print('Unique words in French dataset: {}'.format(set_of_unique_fr_words))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1821465f0dc6f29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e89ecf9ca6c4898d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get all words in each dataset with their frequency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c45c367e2438a08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get all words in each dataset\n",
    "english_words = []\n",
    "french_words = []\n",
    "\n",
    "# Split each sentence into words and add them to the list of words\n",
    "df_both['english'].apply(lambda x: english_words.extend(x.split()))\n",
    "df_both['french'].apply(lambda x: french_words.extend(x.split()))\n",
    "\n",
    "# Get frequency of each word in each dataset\n",
    "english_words_counts = Counter(english_words)\n",
    "french_words_counts = Counter(french_words)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f93f62cd49c94fff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort words in each dataset by their frequency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "908b14c4f23d692f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "english_words_counts = sorted(english_words_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "french_words_counts = sorted(french_words_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "english_words = []\n",
    "english_words_freq = []\n",
    "for i in range(len(english_words_counts)):\n",
    "    english_words.append(english_words_counts[i][0])\n",
    "    english_words_freq.append(english_words_counts[i][1])\n",
    "\n",
    "french_words = []\n",
    "french_words_freq = []\n",
    "for i in range(len(french_words_counts)):\n",
    "    french_words.append(french_words_counts[i][0])\n",
    "    french_words_freq.append(french_words_counts[i][1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16169dd46a36c41c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot frequency of words in each dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b5b60c7c3e11cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.bar(x = english_words, y = english_words_freq, title='Frequency of words in English dataset', labels={'x':'Words', 'y':'Frequency'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.bar(x = french_words, y = french_words_freq, title='Frequency of words in French dataset', labels={'x':'Words', 'y':'Frequency'})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c24f3931c6023f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e17efb2f5c7a0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get maximum length of a sentence in each dataset for embedding\n",
    "max_eng_sentence_length = int(df_both['english'].apply(lambda x: len(x.split())).max())\n",
    "max_fr_sentence_length = int(df_both['french'].apply(lambda x: len(x.split())).max())\n",
    "\n",
    "print('Maximum length of a sentence in English dataset: {}'.format(max_eng_sentence_length))\n",
    "print('Maximum length of a sentence in French dataset: {}'.format(max_fr_sentence_length))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17efa20b0dc72e68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_and_pad(text, max_length):\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(char_level=False)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Get sequences and pad them (make all sequences the same length)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded_text = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return tokenizer, sequences, padded_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b3865d3ae0094b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_tokenizer, x_sequences, x_padded_text = tokenize_and_pad(df_both.english, max_eng_sentence_length)\n",
    "y_tokenizer, y_sequences, y_padded_text = tokenize_and_pad(df_both.french, max_fr_sentence_length)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6abf9dadff9cd6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check tokenized and padded data of one sentence from each dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18196cb8c69380c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('English sentence: {}'.format(df_both.english[0]))\n",
    "print('English sentence after tokenization: {}'.format(x_sequences[0]))\n",
    "print('English sentence after padding: {}'.format(x_padded_text[0]))\n",
    "\n",
    "print('\\nFrench sentence: {}'.format(df_both.french[0]))\n",
    "print('French sentence after tokenization: {}'.format(y_sequences[0]))\n",
    "print('French sentence after padding: {}'.format(y_padded_text[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfbeb4017a4f38f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split data into training and testing sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9e55ad747cc537"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_padded_text, y_padded_text, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0821c16945fb1c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build and train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a0ea29e5cea416"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get vocabulary size for each dataset\n",
    "english_vocab_size = len(x_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(y_tokenizer.word_index) + 1\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Embedding(english_vocab_size, 256, input_length=max_eng_sentence_length, mask_zero=True))\n",
    "model.add(LSTM(256))                                # Encoder\n",
    "model.add(RepeatVector(max_fr_sentence_length))     # Decoder - change the dimensionality of the input from 2D to 3D\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b78f64bdec026d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Change the dimensionality of the input from 2D to 3D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82d3871697c324f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "704c76a921d6b416"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5322dfc74a63b3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=1024, epochs=10, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "547d8d3bf03ed014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('models/weights.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14aa5410d7d3ea99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Asses model performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16678d9fdfcb98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d265cd1efe2d2fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_prediction(sentence, _y_tokenizer = y_tokenizer):\n",
    "    # Get predictions for each word in the sentence\n",
    "    predictions = model.predict(sentence)[0]\n",
    "    # Get the word with the highest probability for each prediction\n",
    "    id_to_word = {id: word for word, id in _y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in np.argmax(predictions, 1)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76fe116422c0109f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_to_text(padded_text, tokenizer):\n",
    "    # Get word associated with each id in the padded text\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in padded_text])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "251698edaecb353f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('English sentence: {}'.format(pad_to_text(x_test[i], x_tokenizer)))\n",
    "    print('French sentence: {}'.format(pad_to_text(y_test[i], y_tokenizer)))\n",
    "    print('Predicted French sentence: {}'.format(make_prediction(x_test[i:i+1])))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83b77733e61ca56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95f7005375c7a026"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
